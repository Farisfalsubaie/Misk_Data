---
title: "Feature & Target Engineering"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

# packages required
library(dplyr)
library(ggplot2)
library(rsample)
library(recipes)
library(caret)

# ames data
ames <- AmesHousing::make_ames()


```

Using the Ames dataset and the same approach shown in the last section:

Rather than use a 70% stratified training split, try an 80% unstratified training split. How does your cross-validated results compare?
```{r}
set.seed(123)
split  <- initial_split(ames, prop = 0.8, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```

```{r}
visdat::vis_miss(AmesHousing::ames_raw, cluster = TRUE)
```

Rather than numerically encode the quality and condition features (i.e. step_integer(matches("Qual|Cond|QC|Qu"))), one-hot encode these features. What is the difference in the number of features your training set? Apply the same cross-validated KNN model to this new feature set. How does the performance change? How does the training time change?
```{r}
ames_train %>% select(matches("Qual|Cond|QC|Qu"))
```
```{r}
count(ames_train, Overall_Qual)

	
```

# after we change to 80% we get Overall_Qual differ in  
Fair	33			
Below_Average	184			
Average	655			
Above_Average	608			
Good	466			
Very_Good	277			
Excellent	89			
Very_Excellent	21
# but not in 
Very_Poor	4			
Poor	9	

```{r}
# 1. stratified sampling with the rsample package
set.seed(123)
split  <- initial_split(ames, prop = 0.8, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)

# 2. Feature engineering
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)

# 3. create a resampling method
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
  )

# 4. create a hyperparameter grid search
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# 5. execute grid search with knn model
#    use RMSE as preferred metric
knn_fit <- train(
  blueprint, 
  data = ames_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )

# 6. evaluate results
# print model results
knn_fit

# plot cross validation results
ggplot(knn_fit$results, aes(k, RMSE)) + 
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = scales::dollar)
```




Identify three new step_xxx functions that recipes provides:
Why would these feature engineering steps be applicable to the Ames data?
Apply these feature engineering steps along with the same cross-validated KNN model. How do your results change?
Using the Attrition data set, assess the characterstics of the target and features.
Which target/feature engineering steps should be applied?
Create a feature engineering blueprint and apply a KNN grid search. What is the performance of your model?
